% !TEX program = pdflatex
% ============================================================
% Springer LNCS Skeleton (Single-file) for:
% "Vietnamese Streaming ASR Benchmark & Robustness Scorecard"
% ============================================================
% Requirements:
% - Put this file next to llncs.cls (from Springer LNCS package).
% - Compile with pdflatex (or lualatex) + bibtex if using thebibliography/bib.
% ============================================================

\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multirow}


% Optional: nicer citations; LNCS supports natbib, but keep simple unless needed.
% \usepackage[numbers]{natbib}

\begin{document}

\title{Benchmarking Streaming ASR for Real-time Deployment:\\
A Robustness Scorecard and Error Taxonomy for Vietnamese}

\titlerunning{Benchmarking Vietnamese Streaming ASR}

% Authors
\author{
Quoc Hung NGUYEN\orcidID{0000-0002-6363-0160}\textsuperscript{*}\and
Hoai An THAI\orcidID{0009-0004-4093-522X}\and
Duy Tan NGUYEN\orcidID{0009-0000-2832-8647}\and
Vy LE\orcidID{0009-0003-0387-8302}\and
Thi Thuy Duong NGUYEN\orcidID{0009-0000-3548-2212}
}

\authorrunning{NGUYEN et al.}

\institute{UEH College of Technology and Design, University of Economics Ho Chi Minh City\\
Corresponding author: \email{hungngq@ueh.edu.vn}\\}


\maketitle

\begin{abstract}
Vietnamese ASR is increasingly deployed in production, where offline word error rate (WER) alone is insufficient for model selection under streaming latency and runtime budgets. 
We present a reproducible streaming evaluation framework that standardizes chunking, right context (look-ahead), overlap handling, and metric computation, and reports WER and real-time factor (RTF) under multiple deployment-oriented latency profiles (1200/2400/4000\,ms) across representative model families. 
We evaluate on VIVOS, VLSP2020, Viet YouTube ASR v2, and Speech-MASSIVE\_vie to produce a robustness scorecard under streaming stressors and distribution shifts. 
We further provide a preliminary Vietnamese-specific error taxonomy (numerals, punctuation, named entities, and Vietnamese--English code-switching) to translate benchmark outcomes into deployment guidance.
\keywords{Vietnamese ASR \and Streaming evaluation \and Latency \and Real-time factor \and Robustness}
\end{abstract}

\section{Introduction}

% Problem: Offline WER is not enough; production needs latency/RTF/robustness.
% Context: Vietnamese ASR in industry; streaming constraints.
Vietnamese automatic speech recognition (ASR) is widely used in production \cite{zhuo2025vietasr,hai2022vlsp2021asr}.  
Model selection, however, is still often driven by offline word error rate (WER). 
This is misaligned with streaming deployment, where systems must satisfy latency budgets and limited compute \cite{yao2021wenet,pratap2020scaling}. 
The problem is sharper for Vietnamese because evaluation data often differ from real usage: read speech vs.\ spontaneous and in-the-wild speech \cite{benzeghiba2007variability,hai2022vlsp2021asr}, combined with strong accent variability \cite{vivos2016,vlsp2020asr,le2024phowhisper,dinh2024vimd}.

To facilitate transparent evaluation and reproducibility, we release the full benchmark implementation and configuration.
\footnote{Repository: \url{https://github.com/hoaianthai345/ASR_Vietnamese_Benchmark.git}.}





% Use crisp, testable contributions.
We present a system-oriented benchmark for Vietnamese streaming ASR. Our contributions are:
\begin{itemize}
  \item We define a reproducible streaming evaluation protocol (with a reference implementation) covering chunking, look-ahead, overlap handling, and metric computation.
  \item We report WER, $\Delta$WER (streaming minus offline), and RTF under three streaming latency profiles representative of practical deployment.
  \item We provide a robustness scorecard measuring degradation under streaming stressors and realistic distribution shifts.
  \item We provide a preliminary Vietnamese-specific error taxonomy and summarize practical implications for deployment.
\end{itemize}


\section{Related Work}

ASR evaluation often reports offline word error rate (WER), which reflects transcription accuracy but not the constraints of streaming deployment. In streaming decoding, limited right context and bounded computation can change both accuracy and latency, so the reported WER depends on settings such as chunk size, look-ahead, and overlap. Because these settings are not always described in a consistent way, comparing results across papers can be difficult \cite{zhang2020benchmarking,yao2021wenet}.

Several toolkits report runtime metrics such as latency, real-time factor (RTF), and throughput alongside WER, which helps evaluate deployment trade-offs. Streaming toolkits also expose decoding settings and make it possible to report latency and RTF under controlled configurations \cite{yao2021wenet}. Open leaderboards standardize reporting, but many rankings still emphasize offline accuracy and provide limited support for streaming controls or stress testing \cite{srivastav2025openasr}.

Vietnamese ASR performance on public benchmarks does not always match real-world usage. Many studies still evaluate on clean read speech, while production audio often includes spontaneous speech, in-the-wild recordings, and strong accent variation \cite{vivos2016,vlsp2020asr,le2024phowhisper}. In addition, Vietnamese transcripts are sensitive to text normalization choices such as numerals and punctuation \cite{sproat2001nsw,zhang2019textnorm}. Named entities and Vietnamese--English code-switching further contribute errors that aggregate WER may not capture well \cite{nguyen2019punctuation,liang2023codeswitchner}.

Robustness under distribution shift has been studied using controlled tests with noise, speaking rate variation, reverberation, and domain mismatch \cite{barker2017chime3,ko2015audioaugmentation}. These studies report clear performance drops outside the training domain. However, robustness is rarely evaluated together with streaming constraints. Because streaming decoding operates with a truncated right context, partial hypotheses are revised more frequently, especially under domain mismatch. Existing streaming evaluations and toolkits report latency/RTF and WER but do not jointly analyze robustness under distribution shift \cite{zhang2020benchmarking,yao2021wenet}. We are not aware of prior Vietnamese benchmarks that jointly report WER, RTF, and streaming degradation under realistic shifts. Table~\ref{tab:related-work} summarizes these gaps.

\begin{table}[h]
\caption{Comparison of related ASR benchmarks.}\label{tab:related-work}%
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
Work & Language & Streaming & Latency / RTF & Robustness & Error Analysis & Reproducible \\
\midrule
Representative Streaming ASR models \cite{zhang2020benchmarking}        & Multi      & Yes & Partial & No      & No  & Partial \\
WeNet toolkit \cite{yao2021wenet}               & Multi      & Yes & Yes     & No      & No  & Yes     \\
Open ASR Leaderboard \cite{srivastav2025openasr}        & Multi      & No  & Yes     & No      & No  & Yes     \\
Vietnamese ASR benchmarks \cite{vivos2016,vlsp2020asr}   & Vietnamese & No  & No      & Partial & No  & Partial \\
This work                   & Vietnamese & Yes & Yes     & Yes     & Yes & Yes     \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Evaluation Pipeline and Setup}

The evaluation pipeline aims to support reproducible streaming ASR experiments. It includes four stages: dataset preparation, streaming simulation and inference, metric computation, and analysis. Figure~\ref{fig:pipeline} shows the overall structure.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/pipeline.png}
  \caption{Overview of the evaluation pipeline.}
  \label{fig:pipeline}
\end{figure}

We evaluate models on four public Vietnamese speech datasets. VIVOS is used as a clean read-speech baseline \cite{vivos2016}. VLSP2020 includes spontaneous speech and represents domain shift \cite{vlsp2020asr}. The Vietnamese YouTube ASR Corpus v2 contains in-the-wild recordings across different domains \cite{linhtran92_vietyoutubeasr_v2}. Speech-MASSIVE (Vietnamese) consists of short assistant-style utterances with strict latency requirements \cite{lee2024speechmassive}. 

For latency-sweep experiments, we use fixed 300-utterance subsets per dataset. All audio is resampled to a common rate and converted to mono. Reference transcripts are normalized using a unified procedure.

We evaluate several ASR models with public checkpoints. ChunkFormer is used as the main streaming model in our analysis.
wav2vec2-base (Vietnamese, 250h) is used as a lightweight CTC-based baseline amenable to streaming simulation.
PhoWhisper-large represents a strong Vietnamese-specific offline upper bound, while Whisper-large-v3 serves as a multilingual baseline.
Deployment-oriented variants based on faster-whisper are left as future work due to environment/toolchain compatibility constraints in this run.
No model is retrained or fine-tuned; all results are obtained through inference-only evaluation to isolate the effects of streaming constraints and evaluation protocol choices.

Streaming inference is simulated by segmenting input audio into fixed-size chunks and restricting right context using a configurable look-ahead window.
For streaming-capable models (ChunkFormer and wav2vec2), we evaluate three deployment-oriented profiles:
1200\,ms, 2400\,ms, and 4000\,ms chunk size, each with 200\,ms overlap and 0\,ms look-ahead.
We apply the same chunking and overlap settings to all streaming models. Offline decoding with full context is used as reference.
With 0\,ms look-ahead, the latency proxy equals the chunk size.

We use word error rate (WER) as the main accuracy metric and report 95\%
bootstrap confidence intervals \cite{bisani2004bootstrap,liu2020blockwisebootstrap}.
Runtime efficiency is measured by real-time factor (RTF), defined as inference
time divided by audio duration, and we report throughput when available.
Streaming degradation is the difference between streaming and offline WER.
All runtime numbers are measured on the same hardware.

We document the dataset subsets, streaming settings, normalization rules, and
inference scripts used in our experiments. Runs use fixed random seeds and can
be reproduced with a single command. The repository also supports rerunning
the benchmark with new models or datasets.

\section{Results and Analysis}

Table~\ref{tab:overall-wer} lists WER for each model on each dataset under offline
and streaming decoding. We report 95\% bootstrap confidence intervals.
Offline results provide the full-context baseline, and the gap to streaming varies
across datasets and model families.

PhoWhisper-large has low WER on VIVOS but degrades on VLSP2020 (Table~\ref{tab:overall-wer}).
This pattern is consistent with a domain shift from read speech to spontaneous, multi-speaker audio, and it may also be affected by decoding choices and acoustic variability.

\begin{table}[h]
\caption{Overall WER (\%) with 95\% confidence intervals. Lower is better. N/A denotes offline-only models.}\label{tab:overall-wer}%
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Model & Dataset & Offline WER (95\% CI) & Streaming WER (95\% CI) & $\Delta$WER \\
\midrule
\multirow{4}{*}{ChunkFormer-CTC-large (110M)} 
  & VIVOS               & 3.44 [2.68, 4.39] & 6.17 [4.85, 7.57] & 2.74 \\
  & VLSP2020            & 3.83 [3.26, 4.45] & 50.64 [46.27, 54.42] & 46.81 \\
  & Viet YouTube ASR v2 & 4.48 [3.83, 5.18] & 5.44 [4.45, 6.83] & 0.96 \\
  & Speech-MASSIVE\_vie & 18.78 [15.49, 22.28] & 21.38 [17.75, 24.98] & 2.60 \\
\midrule
\multirow{4}{*}{wav2vec2-base-vi (95M)} 
  & VIVOS               & 8.88 [7.69, 10.22] & 12.32 [10.54, 13.95] & 3.44 \\
  & VLSP2020            & 10.09 [9.14, 11.35] & 52.74 [49.03, 56.17] & 42.65 \\
  & Viet YouTube ASR v2 & 7.90 [6.81, 8.86] & 8.31 [7.02, 9.51] & 0.40 \\
  & Speech-MASSIVE\_vie & 27.71 [24.12, 31.29] & 30.38 [26.95, 34.14] & 2.68 \\
\midrule
\multirow{4}{*}{PhoWhisper-large (1.55B; offline)} 
  & VIVOS               & 3.01 [2.32, 3.80] & N/A & N/A \\
  & VLSP2020            & 17.49 [13.74, 21.60] & N/A & N/A \\
  & Viet YouTube ASR v2 & 10.55 [8.80, 12.69] & N/A & N/A \\
  & Speech-MASSIVE\_vie & 17.85 [14.60, 20.78] & N/A & N/A \\
\midrule
\multirow{4}{*}{Whisper-large-v3 (1.55B; offline)} 
  & VIVOS               & 9.49 [8.27, 11.11] & N/A & N/A \\
  & VLSP2020            & 24.91 [21.89, 28.25] & N/A & N/A \\
  & Viet YouTube ASR v2 & 36.27 [30.08, 42.86] & N/A & N/A \\
  & Speech-MASSIVE\_vie & 22.39 [18.98, 25.82] & N/A & N/A \\
\bottomrule
\end{tabular}%
}
\end{table}


Table~\ref{tab:rtf} reports real-time factor (RTF) and throughput measured
under the same hardware setup. Larger models achieve lower offline WER,
but require more computation. Deployment decisions should consider this trade-off.

\begin{table}[h]
\caption{Runtime efficiency measured by real-time factor (RTF) and throughput on the same hardware. Lower RTF and higher throughput indicate better performance.}\label{tab:rtf}%
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
Model & RTF (median [IQR]) & Throughput (xRT) & Hardware \\
\midrule
ChunkFormer-CTC-large (110M)      & 0.0111 [0.0016] & 90.1 & 1$\times$NVIDIA H100 80GB \\
wav2vec2-base-vi (95M)            & 0.0059 [0.0009] & 169.5 & 1$\times$NVIDIA H100 80GB \\
PhoWhisper-large (1.55B)          & 0.1034 [0.0170] & 9.7 & 1$\times$NVIDIA H100 80GB \\
Whisper-large-v3 (1.55B)          & 0.0903 [0.0186] & 11.1 & 1$\times$NVIDIA H100 80GB \\
\bottomrule
\end{tabular}%
}
\end{table}


Figure~\ref{fig:wer-latency} shows streaming WER across three latency settings.
We use chunk sizes of 1200, 2400, and 4000\,ms with 200\,ms overlap and no look-ahead.

\subsection{Latency--Accuracy Frontier Analysis}

Figure~\ref{fig:wer-latency} shows that increasing chunk size reduces streaming WER for both models. From 1200\,ms to 4000\,ms, ChunkFormer improves from 34.8\% to 20.9\%, and wav2vec2 improves from 43.5\% to 25.9\%. Latency setting has a strong impact on performance.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/fig_frontier_wer_latency.png}
\caption{Streaming WER with 95\% CI across 1200, 2400, and 4000\,ms settings.}
\label{fig:wer-latency}
\end{figure}

Figure~\ref{fig:delta-wer} shows $\Delta$WER across the same latency settings.
For both models, $\Delta$WER decreases as chunk size increases. Larger chunks provide more context. VLSP2020 remains the most difficult dataset. Degradation is still large at 4000\,ms.

\subsection{VLSP2020 Failure Analysis}

The degradation on VLSP2020 is consistent across settings. At 4000\,ms, both models still exceed 50\% streaming WER, while offline WER remains low. VLSP2020 also shows more chunks per utterance and a higher change rate than other datasets. At 1200\,ms, the number of chunks increases and the change rate rises further. These results point to unstable decoding under multi-chunk processing. This instability poses a risk for long or variable utterances.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/fig_frontier_delta_latency.png}
\caption{Latency sweep for streaming degradation ($\Delta$WER in percentage points). Lower is better.}
\label{fig:delta-wer}
\end{figure}

\subsection{Statistical Significance of $\Delta$WER}

We perform paired bootstrap tests over utterances (\(n=5000\) resamples per run) to verify whether streaming degradation is statistically different from zero.
Across the 8 streaming runs (4 datasets \(\times\) 2 models), 7 runs show significant degradation with one-sided \(p<0.05\).
The only non-significant case is \textit{viet\_youtube\_asr\_v2\_300 + wav2vec2-base-vi}, where \(\Delta\)WER is small (0.40 percentage points), the 95\% CI crosses zero ([-0.59, 1.62] pp), and \(p=0.2628\).
All but one \(\Delta\)WER cases are statistically significant.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/fig_delta_significance.png}
\caption{Paired-bootstrap significance summary of streaming degradation. Left: \(\Delta\)WER (pp) with 95\% CI. Right: one-sided \(p\)-values with threshold \(p=0.05\).}
\label{fig:delta-significance}
\end{figure}

\subsection{Streaming Stability}

Following prior work on incremental and streaming ASR evaluation that
emphasizes hypothesis stability under partial decoding, we analyze
incremental-hypothesis stability using change rate and edit-overhead metrics
\cite{selfridge2011stability,ma2020emformer}.
Change rate measures the proportion of tokens revised between successive
partial hypotheses, while edit-overhead quantifies the cumulative edit
operations required to reach the final hypothesis relative to its length.
Figure~\ref{fig:stability} shows that stability improves markedly as latency increases:
mean change rate drops from approximately 81\% (1200\,ms) to 31\% (4000\,ms),
and mean edit overhead drops from approximately 1.55 to 0.44.
VLSP2020 is consistently the least stable condition, with change rates around 0.88--0.92 at 1200/2400\,ms and still around 0.80 at 4000\,ms.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/fig_stability_change_rate_heatmap.png}
\caption{Streaming stability heatmap (hypothesis change rate, \%) across datasets and latency profiles.}
\label{fig:stability}
\end{figure}

Table~\ref{tab:robustness} reports accuracy degradation under distribution shifts,
including spontaneous speech, in-the-wild recordings, and assistant-style utterances.
For each condition, we report both absolute and relative shifts against the clean read-speech baseline (VIVOS) at 4000\,ms.
Reporting both metrics avoids over-emphasis from ratio-only views when the clean baseline is small.

\subsection{Robustness with Absolute and Relative Shift}

To make robustness comparison explicit, we define a model-wise robustness index at fixed latency:
\[
\mathrm{RI}(d,m)=\frac{\mathrm{WER}_{\text{stream}}(d,m)}{\mathrm{WER}_{\text{stream}}(\text{VIVOS},m)}.
\]
This is equivalent to \(1+\frac{\text{Relative Shift}}{100}\), and complements absolute shifts (percentage points).

\begin{table}[h]
\caption{Robustness scorecard at 4000\,ms streaming profile. Absolute shift is in percentage points (pp) from VIVOS. Relative shift is \% change from VIVOS; negative values indicate improvement.}\label{tab:robustness}%
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}llccc@{}}
\toprule
Model & Dataset & WER (\%) & Absolute Shift (pp) & Relative Shift (\%) \\
\midrule
\multirow{4}{*}{ChunkFormer-CTC-large}
 & VIVOS               & 6.17  & 0.00   & 0.0 \\
 & VLSP2020            & 50.64 & +44.46 & +720.2 \\
 & Viet YouTube ASR v2 & 5.44  & -0.73  & -11.9 \\
 & Speech-MASSIVE\_vie & 21.38 & +15.21 & +246.3 \\
\midrule
\multirow{4}{*}{wav2vec2-base-vi}
 & VIVOS               & 12.32 & 0.00   & 0.0 \\
 & VLSP2020            & 52.74 & +40.42 & +328.2 \\
 & Viet YouTube ASR v2 & 8.31  & -4.01  & -32.6 \\
 & Speech-MASSIVE\_vie & 30.38 & +18.07 & +146.7 \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/fig_robustness_abs_rel_4000.png}
\caption{Robustness comparison at 4000\,ms profile: absolute and relative shifts from VIVOS.}
\label{fig:robustness-abs-rel}
\end{figure}

Based on the observed latency-sweep results, we derive deployment-oriented insights.
ChunkFormer consistently outperforms wav2vec2 across all datasets and all three latency profiles, but both models degrade substantially on VLSP2020. Streaming evaluation under fixed latency settings should be reported together with offline WER.

\section{Conclusion and Future Work}

We propose a system-oriented benchmark for Vietnamese streaming ASR that moves beyond offline accuracy-centric evaluation.
We introduce a unified evaluation pipeline for streaming ASR. We evaluate models under practical latency constraints and report both accuracy and runtime cost.
Our results demonstrate that model selection based solely on offline WER can be misleading in deployment scenarios,
as streaming constraints and runtime cost substantially alter the relative performance of different model families.

Through latency--profile comparisons of WER, $\Delta$WER, and RTF, we identify configurations with favorable accuracy--efficiency trade--offs.
In addition, the proposed robustness scorecard reveals that streaming constraints often amplify performance degradation under distribution shifts,
including spontaneous speech and in-the-wild conditions that are common in Vietnamese applications.
Our preliminary Vietnamese-specific error taxonomy (numerals, abbreviations, named entities, and code-switching) exposes likely failure patterns and motivates future quantitative error breakdown.

This work focuses on evaluation rather than model training, and several limitations remain.
We do not retrain or fine-tune models under streaming constraints, and robustness analysis is limited to a fixed set of datasets and stress conditions.
Although we evaluate three practical latency profiles, we do not claim an exhaustive continuous latency--accuracy frontier.
RTF results are measured on a single H100 80GB setup and should be interpreted as throughput upper bounds rather than edge-deployment guarantees.
Future work may extend this benchmark by incorporating additional streaming architectures, low-resource adaptation techniques,
explicit latency-budget sweeps, and more fine-grained latency measurements that account for end-to-end system effects.
We also plan to expand the robustness analysis to cover a broader range of acoustic and linguistic variations,
and to integrate the benchmark into an open, continuously updated evaluation platform for Vietnamese ASR.

By providing an open, reproducible, and deployment-aligned evaluation framework,
we hope this work serves as a reference point for future research and practical system development in Vietnamese streaming ASR.

\section*{Acknowledgement}
This research is funded by the University of Economics Ho Chi Minh City (UEH).

\bibliographystyle{splncs04}
\bibliography{references}



\end{document}
